{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "stopwords = set(ENGLISH_STOP_WORDS)\n",
    "stopwords.add(\"tomorrow\")\n",
    "stopwords.add(\"day\")\n",
    "stopwords.add(\"http\")\n",
    "stopwords.add(\"3rd\")\n",
    "stopwords.add(\"u002c\")\n",
    "stopwords.add(\"time\")\n",
    "stopwords.add(\"1st\")\n",
    "stopwords.add(\"today\")\n",
    "stopwords.add(\"make\")\n",
    "stopwords.add(\"think\")\n",
    "stopwords.add(\"u2019m\")\n",
    "stopwords.add(\"saturday\")\n",
    "stopwords.add(\"tuesday\")\n",
    "stopwords.add(\"wednesday\")\n",
    "stopwords.add(\"people\")\n",
    "stopwords.add(\"did\")\n",
    "stopwords.add(\"will\")\n",
    "stopwords.add(\"said\")\n",
    "stopwords.add(\"say\")\n",
    "stopwords.add(\"says\")\n",
    "stopwords.add(\"it\")\n",
    "stopwords.add(\"they\")\n",
    "stopwords.add(\"are\")\n",
    "stopwords.add(\"that\")\n",
    "stopwords.add(\"saying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ REMOVE SPECIAL CHARS AND STOPWORDS from train\n",
    "trainData = pd.read_csv('train2017.tsv', sep=\"\\t\", encoding = \"utf-8\")\n",
    "\n",
    "stemmed = []\n",
    "removed = []  # cleaned tweets\n",
    "tokens = []\n",
    "similar_words = []\n",
    "vect = []\n",
    "tokenized_tweet = []\n",
    "\n",
    "for i in range(0, trainData.shape[0]):\n",
    "\tremoved.append(re.sub(r\"[^a-zA-Z0-9]+\", ' ', trainData.loc[i][3]))\n",
    "\n",
    "\tword_tokens = word_tokenize(removed[i])\n",
    "\tfiltered_sentence = []\n",
    "\tfor w in word_tokens: \n",
    "\t\tif w.lower() not in stopwords: \n",
    "\t\t\tfiltered_sentence.append(w.lower())\n",
    "\n",
    "\t\t\t# STEM\n",
    "\t\t\tporter = PorterStemmer()\n",
    "\t\t\tstemmed.extend([porter.stem(w.lower())]) \n",
    "\ttokens.append(filtered_sentence)\n",
    "\tremoved[i] = ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ REMOVE SPECIAL CHARS AND STOPWORDS from test\n",
    "testData = pd.read_csv('test2017.tsv', sep=\"\\t\", encoding = \"utf-8\")\n",
    "\n",
    "stemmed2 = []\n",
    "removed2 = []  # cleaned tweets\n",
    "tokens2 = []\n",
    "similar_words2 = []\n",
    "vect2 = []\n",
    "tokenized_tweet2 = []\n",
    "\n",
    "for i in range(0, testData.shape[0]):\n",
    "\tremoved2.append(re.sub(r\"[^a-zA-Z0-9]+\", ' ', testData.loc[i][3]))\n",
    "\n",
    "\tword_tokens2 = word_tokenize(removed2[i])\n",
    "\tfiltered_sentence2 = []\n",
    "\tfor w in word_tokens2: \n",
    "\t\tif w.lower() not in stopwords: \n",
    "\t\t\tfiltered_sentence2.append(w.lower())\n",
    "\n",
    "\t\t\t# STEM\n",
    "\t\t\tporter2 = PorterStemmer()\n",
    "\t\t\tstemmed2.extend([porter2.stem(w.lower())]) \n",
    "\ttokens2.append(filtered_sentence2)\n",
    "\tremoved2[i] = ' '.join(filtered_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE WORD EMBEDDINGS\n",
    "\n",
    "# ------------------ w2v train ------------------\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokens,\n",
    "            size=200, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34)\n",
    "\n",
    "model_w2v.train(tokens, total_examples= len(removed), epochs=20)\n",
    "\n",
    "model_w2v.save('embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD WORD EMBEDDINGS\n",
    "\n",
    "new_model = gensim.models.Word2Vec.load('embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFFIN LEXICA\n",
    "\n",
    "affin = pd.read_csv('affin.txt', sep=\"\\t\", encoding = \"utf-8\")\n",
    "\n",
    "affin_lexica = []\n",
    "for i in range(0, affin.shape[0]):\n",
    "\tvals = {}\n",
    "\tvals['word'] = affin.loc[i][0]\n",
    "\tvals['vector'] = affin.loc[i][1]\n",
    "\taffin_lexica.append(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athina/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.65066015e-01 -1.01045091e-01  1.14212856e-01  5.77932309e-02\n",
      " -6.96794328e-02  1.10225257e-01 -2.03491599e-01 -3.38839713e-02\n",
      " -2.75364742e-01  2.30094700e-01  3.83810033e-01  1.66269127e-01\n",
      " -2.11221636e-01  1.85103175e-01  4.87432075e-02 -1.14479540e-01\n",
      " -7.34377195e-02 -8.99893567e-02  1.29985578e-01  3.85854447e-01\n",
      " -1.68923436e-01 -1.57861213e-01 -2.57856622e-01  1.61487778e-02\n",
      "  1.97000790e-02  5.84129178e-02 -5.44269140e-01 -1.16109610e-01\n",
      "  8.58304368e-02  1.56476113e-01 -6.68982911e-02  2.25978569e-01\n",
      "  1.60397035e-01  2.10748809e-01 -1.15914147e-01 -2.83041410e-03\n",
      " -1.89363313e-02  2.64590528e-01 -1.65608092e-01 -3.59845604e-02\n",
      "  1.22831022e-01  3.37625882e-01 -2.13334709e-01  1.71807877e-02\n",
      " -1.57130579e-01  8.22499549e-02  1.44180584e-01  4.50663308e-02\n",
      "  1.29118836e-01 -6.73709634e-02 -1.98930364e-02 -1.22538634e-01\n",
      " -1.50742502e-02  3.62732334e-01  9.53227588e-03 -8.06608157e-03\n",
      " -2.44499243e-01 -1.24881596e-01 -1.26082501e-01  2.24337077e-01\n",
      "  2.88640742e-01 -5.15868174e-02  2.49835735e-01 -9.00870700e-02\n",
      "  1.61097271e-01  1.17308483e-01  1.82559672e-01 -1.74039722e-02\n",
      " -4.83901554e-02 -1.49322449e-01 -2.54801585e-02  2.10797829e-01\n",
      "  3.17684196e-01  7.50594609e-02 -1.62791373e-02 -2.66538508e-01\n",
      " -7.96266541e-02  1.61827313e-02 -1.26945053e-01  4.63023615e-02\n",
      " -1.90728263e-01 -4.13303989e-01  6.03212809e-02  1.15847560e-01\n",
      " -1.96545883e-01  1.66955213e-01 -2.79299447e-02 -2.18659480e-02\n",
      " -3.03805385e-02 -1.40012769e-01 -2.19813886e-01  8.59166682e-03\n",
      "  1.93316121e-01  1.08581686e-02 -1.44142734e-01 -1.57175341e-02\n",
      " -5.89859146e-02 -1.58883448e-01 -4.67649294e-02 -3.34209155e-01\n",
      " -1.33422700e-01  3.84810514e-04  3.88793077e-01  9.70905246e-02\n",
      " -6.50783900e-02  1.73705603e-03  1.01250398e-01 -4.41044812e-01\n",
      "  9.59018360e-02  3.27616515e-02 -1.06551546e-01 -1.32897166e-01\n",
      " -1.25057965e-01  4.65759634e-01 -9.99260268e-02  2.35082152e-01\n",
      " -1.48339157e-02 -1.84062878e-01  4.27007084e-02  1.09315465e-01\n",
      " -1.98558320e-01 -4.06812287e-01  8.08166989e-02 -9.30894941e-03\n",
      " -4.42892648e-01 -7.06585103e-02 -1.51860955e-01  3.33079958e-01\n",
      " -1.18994537e-01  1.84079015e-01 -9.52478594e-02  1.35537391e-02\n",
      " -1.68091621e-01  3.48973975e-01 -2.20914838e-01  9.80563536e-02\n",
      "  1.54163553e-01  2.39601457e-01  1.50820870e-01 -2.82494221e-01\n",
      "  1.47455679e-01  2.46907590e-01  3.82242490e-02  8.47848360e-02\n",
      " -7.22074676e-02 -1.23634322e-01 -4.70200922e-01 -4.99483986e-02\n",
      "  4.63511561e-02  2.35403413e-01 -1.28044792e-01 -2.46134839e-02\n",
      "  2.24000869e-01 -1.60599128e-01 -2.53175087e-01 -2.16623627e-02\n",
      " -4.19804118e-01  4.73628901e-02 -1.04943460e-01  5.06515528e-03\n",
      "  1.06766151e-01 -3.00116579e-01  7.37475662e-02  3.06146502e-01\n",
      " -2.94547008e-01 -2.66501088e-01  1.73188693e-01  3.08305640e-01\n",
      " -2.64259422e-02  3.50727064e-01  1.36221071e-01  3.71635526e-01\n",
      " -1.93425024e-01 -2.40114629e-02  2.99203067e-01  6.24946803e-02\n",
      " -2.26415212e-01  2.72852525e-01  1.57096095e-02 -1.40234759e-01\n",
      "  1.05709089e-02 -3.37842280e-02 -1.64424056e-01  4.01176560e-02\n",
      "  1.07735073e-01 -2.10221199e-01 -5.81994190e-02  2.03085316e-01\n",
      "  1.52860547e-02 -2.17581522e-01  1.06508880e-01 -3.53726280e-01\n",
      "  5.84472479e-02 -5.68432488e-02 -1.54565608e-01  2.29970302e-02\n",
      "  1.34921630e-01  1.26259399e-01  1.64830252e-01 -1.01114075e-01\n",
      "  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# TWEET VECTORS\n",
    "\n",
    "# ------------------ w2v train ------------------\n",
    "\n",
    "vec = []\n",
    "vec_tweet = []   # list with the vectors of each tweet\n",
    "\n",
    "for i in range(0, trainData.shape[0]):\n",
    "\tvec = np.zeros(201)  # pos 201 for lexica\n",
    "\twcount = len(removed[i].split())\n",
    "\tfor word in removed[i].split():\n",
    "\t\tfor j in range(0,200):\n",
    "\t\t\tif word in new_model.wv.vocab:\n",
    "\t\t\t\tvec[j] += (new_model[word][j] / wcount)\n",
    "\t\tif word in affin_lexica:\n",
    "\t\t\tvec[200] = affin_lexica[word]\n",
    "\tvec_tweet.append(vec)\n",
    "\n",
    "# print (vec_tweet[0])\n",
    "\n",
    "# EXPORT TWEET VECTORS TO FILE\n",
    "tv_output = open(\"tweetvectors.pkl\", \"wb\")\n",
    "pickle.dump(vec_tweet, tv_output)\n",
    "tv_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TWEET VECTORS for train\n",
    "\n",
    "tv_file = open(\"tweetvectors.pkl\", \"rb\")\n",
    "vec_tweet = pickle.load(tv_file)\n",
    "tv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athina/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# TWEET VECTORS\n",
    "\n",
    "# ------------------ w2v test ------------------\n",
    "\n",
    "vec2 = []\n",
    "vec_tweet_test = []    # list with the vectors of each tweet\n",
    "\n",
    "for i in range(0, testData.shape[0]):\n",
    "\tvec2 = np.zeros(201)\n",
    "\twcount = len(removed2[i].split())\n",
    "\tfor word in removed2[i].split():\n",
    "\t\tfor j in range(0,200):\n",
    "\t\t\tif word in new_model.wv.vocab:\n",
    "\t\t\t\tvec2[j] += (new_model[word][j] / wcount)\n",
    "\t\tif word in affin_lexica:\n",
    "\t\t\tvec2[200] = affin_lexica[word]\n",
    "\tvec_tweet_test.append(vec2)\n",
    "\n",
    "# EXPORT TWEET VECTORS TO FILE\n",
    "tv2_output = open(\"tweetvectorstest.pkl\", \"wb\")\n",
    "pickle.dump(vec_tweet_test, tv2_output)\n",
    "tv2_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TWEET VECTORS for test\n",
    "\n",
    "tv2_file = open(\"tweetvectorstest.pkl\", \"rb\")\n",
    "vec_tweet_test = pickle.load(tv2_file)\n",
    "tv2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAG OF WORDS\n",
    "\n",
    "# ------------------ train ------------------\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "bow_xtrain = bow_vectorizer.fit_transform(removed)\n",
    "\n",
    "# ------------------ test ------------------\n",
    "\n",
    "bow_vectorizer2 = CountVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "bow_xtrain2 = bow_vectorizer2.fit_transform(removed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT BAG TO FILE\n",
    "\n",
    "# ------------------ train ------------------\n",
    "\n",
    "bow_output = open(\"bagofwords.pkl\", \"wb\")\n",
    "pickle.dump(bow_xtrain, bow_output)\n",
    "bow_output.close()\n",
    "\n",
    "# ------------------ test ------------------\n",
    "\n",
    "bow_output2 = open(\"bagofwords_test.pkl\", \"wb\")\n",
    "pickle.dump(bow_xtrain2, bow_output2)\n",
    "bow_output2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD BAG\n",
    "\n",
    "# ------------------ train ------------------\n",
    "\n",
    "bow_file = open(\"bagofwords.pkl\", \"rb\")\n",
    "bag = pickle.load(bow_file)\n",
    "bow_file.close()\n",
    "\n",
    "bow_xtrain = bag\n",
    "features = bow_vectorizer.get_feature_names()\n",
    "bag = bag.toarray()\n",
    "bow_df = pd.DataFrame(bag, columns = features)\n",
    "\n",
    "bow_xtrain = bow_xtrain.toarray()\n",
    "\n",
    "# ------------------ test ------------------\n",
    "\n",
    "bow_file2 = open(\"bagofwords_test.pkl\", \"rb\")\n",
    "bag2 = pickle.load(bow_file2)\n",
    "bow_file2.close()\n",
    "\n",
    "bow_xtrain_test = bag2\n",
    "features2 = bow_vectorizer2.get_feature_names()\n",
    "bag2 = bag2.toarray()\n",
    "bow_df2 = pd.DataFrame(bag2, columns = features2)\n",
    "\n",
    "bow_xtrain_test = bow_xtrain_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "# ------------------ train ------------------\n",
    "\n",
    "s = []\n",
    "for i in range(0,trainData.shape[0]):\n",
    "\ts.extend([removed[i]])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf = tfidf_vectorizer.fit_transform(s)\n",
    "tfidf = tfidf.toarray()\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_df = pd.DataFrame(np.round(tfidf, 3), columns = features)\n",
    "\n",
    "# ------------------ test ------------------\n",
    "\n",
    "s2 = []\n",
    "for i in range(0,testData.shape[0]):\n",
    "\ts2.extend([removed2[i]])\n",
    "\n",
    "tfidf_vectorizer2 = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf2 = tfidf_vectorizer2.fit_transform(s2)\n",
    "tfidf2 = tfidf2.toarray()\n",
    "features2 = tfidf_vectorizer2.get_feature_names()\n",
    "tfidf_df2 = pd.DataFrame(np.round(tfidf2, 3), columns = features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT TF_IDF TO FILE\n",
    "\n",
    "# ------------------ train ------------------\n",
    "\n",
    "tfidf_output = open(\"tfidf.pkl\", \"wb\")\n",
    "pickle.dump(tfidf_df, tfidf_output)\n",
    "tfidf_output.close()\n",
    "\n",
    "# ------------------ test ------------------\n",
    "\n",
    "tfidf_output2 = open(\"tfidf_test.pkl\", \"wb\")\n",
    "pickle.dump(tfidf_df2, tfidf_output2)\n",
    "tfidf_output2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TF_IDF\n",
    "\n",
    "# ------------------ train ------------------\n",
    "\n",
    "tfidf_file = open(\"tfidf.pkl\", \"rb\")\n",
    "tf = pickle.load(tfidf_file)\n",
    "tfidf_file.close()\n",
    "\n",
    "# ------------------ test ------------------\n",
    "\n",
    "tfidf_file2 = open(\"tfidf_test.pkl\", \"rb\")\n",
    "tf_test = pickle.load(tfidf_file2)\n",
    "tfidf_file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ LABELS\n",
    "\n",
    "labels = []\n",
    "for i in range(0, trainData.shape[0]):\n",
    "\tlabels.append(trainData.loc[i][2])\n",
    "\n",
    "testLabels = pd.read_csv('SemEval2017_task4_subtaskA_test_english_gold.txt', sep=\"\\t\", encoding = \"utf-8\")\n",
    "labels_test = []\n",
    "for i in range(0, testLabels.shape[0]):\n",
    "\tlabels_test.append(testLabels.loc[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN W2V ONLY FOR TRAIN (train_test_split)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vec_tweet, labels, test_size=0.4, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN W2V FOR TEST\n",
    "\n",
    "X_train = vec_tweet\n",
    "y_train = labels\n",
    "X_test = vec_tweet_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5145322803875275\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "# make predictions on the testing set\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47260441260278435\n"
     ]
    }
   ],
   "source": [
    "# Repeat for KNN with K=5:\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM W2V ONLY FOR TRAIN (train_test_split)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vec_tweet, labels, random_state=42, test_size=0.2) #input for this method is any array of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM W2V FOR TEST\n",
    "\n",
    "X_train = vec_tweet\n",
    "y_train = labels\n",
    "X_test = vec_tweet_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5126597736709273\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc = svc.fit(X_train, y_train)\n",
    "prediction = svc.predict(X_test) #predict on the validation set\n",
    "print (f1_score(y_test, prediction, average='micro')) #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN BAG-OF-WORDS ONLY FOR TRAIN (train_test_split)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_xtrain, labels, test_size=0.4, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN BAG-OF-WORDS FOR TEST\n",
    "\n",
    "X_train = bow_xtrain\n",
    "y_train = labels\n",
    "X_test = bow_xtrain_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.393552063828055\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "# make predictions on the testing set\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'negative' 'neutral' ... 'neutral' 'neutral' 'neutral']\n",
      "0.3998208906618904\n"
     ]
    }
   ],
   "source": [
    "# Repeat for KNN with K=5:\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "# print (y_pred)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM BAG-OF-WORDS ONLY FOR TRAIN (train_test_split)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_xtrain, labels, random_state=42, test_size=0.2) #input for this method is any array of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM BAG-OF-WORDS FOR TEST\n",
    "\n",
    "X_train = bow_xtrain\n",
    "y_train = labels\n",
    "X_test = bow_xtrain_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc = svc.fit(X_train, y_train)\n",
    "prediction = svc.predict(X_test) #predict on the validation set\n",
    "print (f1_score(y_test, prediction, average='micro')) #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN TF-IDF ONLY FOR TRAIN (train_test_split)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf, labels, test_size=0.4, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN TF-IDF FOR TEST\n",
    "\n",
    "X_train = tf\n",
    "y_train = labels\n",
    "X_test = tf_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "# make predictions on the testing set\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for KNN with K=5:\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "# print (y_pred)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM TF-IDF ONLY FOR TRAIN (train_test_split)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf, labels, random_state=42, test_size=0.2) #input for this method is any array of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM TF-IDF FOR TEST\n",
    "\n",
    "X_train = tf\n",
    "y_train = labels\n",
    "X_test = tf_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc = svc.fit(X_train, y_train)\n",
    "prediction = svc.predict(X_test) #predict on the validation set\n",
    "print (f1_score(y_test, prediction, average='micro')) #evaluate on the validation set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
